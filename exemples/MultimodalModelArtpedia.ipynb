{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d609a12-eeed-411a-8a58-c0844d2ea11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from lavis.models.multimodal_models.modelmultimodal import Multimodal\n",
    "from lavis.common.config import Config\n",
    "from lavis.processors.clip_processors import ClipImageEvalProcessor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a499d48-e0f8-4eba-8631-f019e36e834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'arch': 'clip', 'model_type': 'ViT-L-14-336', 'pretrained': 'openai'}, 'preprocess': {'vis_processor': {'eval': {'name': 'clip_image_eval', 'image_size': 336}}}}\n"
     ]
    }
   ],
   "source": [
    "config_path = \"lavis/configs/models/clip_vit_large14_336.yaml\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "print(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97dccf65-d9cd-46ed-b9f0-6909bc780bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Multimodal(\n",
    "    embed_dim=768,\n",
    "    vision_cfg={\n",
    "        \"image_size\": 336,\n",
    "        \"patch_size\": 14,\n",
    "        \"width\": 1024,\n",
    "        \"layers\": 24\n",
    "    },\n",
    "    text_cfg={\n",
    "        \"context_length\": 77,\n",
    "        \"vocab_size\": 49408,\n",
    "        \"width\": 768,\n",
    "        \"layers\": 12,\n",
    "        \"heads\": 12\n",
    "    },\n",
    "    quick_gelu=True,\n",
    "    add_cls_token=True,\n",
    ")\n",
    "model.to(device)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38247a57-4193-4ad1-96d3-2e490be96a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_dict dict_keys(['model', 'optimizer', 'config', 'scaler', 'epoch'])\n",
      "missing []\n",
      "unexpect []\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"../../ArtpediaClassif/20250410100656/checkpoint_best.pth\"\n",
    "state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "missing, unexpected = model.load_state_dict(state_dict[\"model\"], strict=False)\n",
    "\n",
    "model.eval()\n",
    "print(\"state_dict\", state_dict.keys())\n",
    "print(\"missing\", missing)\n",
    "print(\"unexpect\", unexpected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccdeb4fd-70c7-401f-8045-933c5630a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../../model/imgs/maria.jpg\"\n",
    "image = ClipImageEvalProcessor(image_size=336)(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "text_input = [\"This a picture of a woman which old a baby in her arms. Around her there is four people\"]\n",
    "\n",
    "samples = {\n",
    "    \"image\": image,\n",
    "    \"text_input\": text_input,\n",
    "    \"label\": torch.tensor([1]).to(device)  # 1 visuel, 0 contextuel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89664a9f-5ab4-4102-a77e-7bc4b9d7aa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this -> 0\n",
      "a -> 0\n",
      "picture -> 0\n",
      "of -> 0\n",
      "a -> 0\n",
      "woman -> 1\n",
      "which -> 1\n",
      "old -> 0\n",
      "a -> 0\n",
      "baby -> 1\n",
      "in -> 1\n",
      "her -> 1\n",
      "arms -> 1\n",
      ". -> 1\n",
      "around -> 1\n",
      "her -> 1\n",
      "there -> 1\n",
      "is -> 1\n",
      "four -> 0\n",
      "people -> 1\n",
      "\n",
      "Sentence -> 1\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.predict(samples)\n",
    "token=output[\"token\"].squeeze()\n",
    "prediction = output[\"predictions\"].squeeze()[token!=0][1:-1].tolist()\n",
    "words = model.detokenizer(token)\n",
    "\n",
    "for word, pred in zip(words, prediction):\n",
    "    print(word,\"->\", pred)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Sentence ->\", round(np.mean(prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046cf7fb-9fd9-4271-959c-e9db4ab782f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
